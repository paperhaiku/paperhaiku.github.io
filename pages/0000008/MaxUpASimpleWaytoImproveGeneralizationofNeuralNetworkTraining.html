<html>

	<head>
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
		<meta charset="utf-8">
	    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
		<link rel="stylesheet" href="../../haiku.css">
		<title>MaxUp: A Simple Way to Improve Generalization of Neural Network Training | Paper Haiku</title>
		<link rel="shortcut icon" type="image/png" href="../../favicon.png"/>

		<script async src="https://www.googletagmanager.com/gtag/js?id=G-2VZT87J343"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'G-2VZT87J343');
		</script>

	</head>


	<body>

		<div class="header">
			<a href="../../index.html" class="heading">
				Paper Haiku
			</a>
			<div class="nav">
				<!-- <a class="nav-link" href="">Contribute</a> -->
				<!-- <a class="nav-link" href="">Request</a> -->
				<a class="nav-link" href="../../about.html">About</a>
			</div>
		</div>

		<div class="content-holder">

			<div class="content">

				<div class="content-title">
					MaxUp: A Simple Way to Improve Generalization of Neural Network Training
				</div>

				<div class="content-body-top">
					<div class="content-body-top-title">
						Haiku
					</div>
					
						To improve generalization,<br>
					
						Select each training sample,<br>
					
						Apply data augmentation on the sample,<br>
					
						<br>
					
						Instead of taking average(loss),<br>
					
						Calculate max(loss over augm. images) for each sample<br>
					
						And average out over all samples.<br>
					
						<br>
					
						Enforces adversarial robustness,<br>
					
						Improves smoothness of loss fn.<br>
					
						At very low compute overhead.<br>
					
				</div>

				<div class="content-body">

					<div class="content-body-left">
						<div class="content-body-left-title">
						Abstract
						</div>
						We propose MaxUp, an embarrassingly simple, highly effective technique for improving the generalization performance of machine learning models, especially deep neural networks. The idea is to generate a set of augmented data with some random perturbations or transforms, and minimize the maximum, or worst case loss over the augmented data. By doing so, we implicitly introduce a smoothness or robustness regularization against the random perturbations, and hence improve the generation performance. For example, in the case of Gaussian perturbation, MaxUp is asymptotically equivalent to using the gradient norm of the loss as a penalty to encourage smoothness. We test MaxUp on a range of tasks, including image classification, language modeling, and adversarial certification, on which MaxUp consistently outperforms the existing best baseline methods, without introducing substantial computational overhead. In particular, we improve ImageNet classification from the state-of-the-art top-1 accuracy 85.5% without extra data to 85.8%.
					</div>

					<div class="content-body-right">
						
						<p class="content-body-right-para">
							<span class="detailname">Tags</span> 
							<br>
							
								Image Classification;&nbsp;
							
								Data Augmentation;&nbsp;
							
								Regularization;&nbsp;
							
						</p>

						<p class="content-body-right-para">
							<span class="detailname">Level</span>
							<br> 
							Advanced				
						</p>

						
						<p class="content-body-right-para">
							<span class="detailname">ImageNet Top-1 Acc</span> 
							<br>
							85.80%
						</p>
						
						<p class="content-body-right-para">
							<span class="detailname">CIFAR-10 Accuracy</span> 
							<br>
							97.18%
						</p>
						
						<p class="content-body-right-para">
							<span class="detailname">CIFAR-100 Accuracy</span> 
							<br>
							82.48%
						</p>
						

						<p class="content-body-right-para">
							<span class="detailname">Paper Authors</span>
							<br>
							
								Chengyue Gong,&nbsp;
							
								Tongzheng Ren,&nbsp;
							
								Mao Ye,&nbsp;
							
								Qiang Liu,&nbsp;
									
						</p>

						<p class="content-body-right-para">
							<span class="detailname">Authors Affiliation</span>
							<br> 
							Unknown
						</p>

						<p class="content-body-right-para">
							<span class="detailname">Publish Date</span>
							<br> 
							20 Feb 2020
						</p>

						<span>
						<a class="read-more-link" target="_blank" href="https://arxiv.org/pdf/2002.09024v1.pdf">
							Paper Link
						</a>
						
						</span>
					</div>

				</div>

				<div class="content-footer">
					Haiku added on: 08 Feb 2021
				</div>
			</div>


			<div class="excerpt-holder">
				<div class="excerpt-title">
					Paper excerpts to justify the haiku
				</div>
				
					<img class="excerpt-image" src="../.././papers/0000008/screenshots/0000008_01.png">
				
					<img class="excerpt-image" src="../.././papers/0000008/screenshots/0000008_02.png">
				
					<img class="excerpt-image" src="../.././papers/0000008/screenshots/0000008_03.png">
				
					<img class="excerpt-image" src="../.././papers/0000008/screenshots/0000008_04.png">
				
					<img class="excerpt-image" src="../.././papers/0000008/screenshots/0000008_05.png">
				
					<img class="excerpt-image" src="../.././papers/0000008/screenshots/0000008_06.png">
				
					<img class="excerpt-image" src="../.././papers/0000008/screenshots/0000008_07.png">
				
			</div>
		</div>


</html>