<html>

	<head>
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
		<meta charset="utf-8">
	    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
		<link rel="stylesheet" href="../../haiku.css">
		<title>Exploring the Limits of Weakly Supervised Pretraining | Paper Haiku</title>
		<link rel="shortcut icon" type="image/png" href="../../favicon.png"/>

		<script async src="https://www.googletagmanager.com/gtag/js?id=G-2VZT87J343"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'G-2VZT87J343');
		</script>

	</head>


	<body>

		<div class="header">
			<a href="../../index.html" class="heading">
				Paper Haiku
			</a>
			<div class="nav">
				<!-- <a class="nav-link" href="">Contribute</a> -->
				<!-- <a class="nav-link" href="">Request</a> -->
				<a class="nav-link" href="../../about.html">About</a>
			</div>
		</div>

		<div class="content-holder">

			<div class="content">

				<div class="content-title">
					Exploring the Limits of Weakly Supervised Pretraining
				</div>

				<div class="content-body-top">
					<div class="content-body-top-title">
						Haiku
					</div>
					
						Sample a noisy dataset,<br>
					
						with 3.5B images,<br>
					
						from instagram.<br>
					
						<br>
					
						Train it on a <br>
					
						big model (ResNeXt-101 32×48d),<br>
					
						with 829M parameters.<br>
					
						<br>
					
						Finetune on Imagenet<br>
					
						Achieve SOTA!<br>
					
						Transfer learning helps.<br>
					
						<br>
					
						Pre-training is in a way is<br>
					
						sophisticated weight initialization,<br>
					
						which improves accuracy when applied.<br>
					
						<br>
					
						Full network finetuning is done by,<br>
					
						Removing FC layer, add new FC layer,<br>
					
						Train full network \w SDG + momentum.<br>
					
						<br>
					
						Feature Transfer is done by,<br>
					
						training L2-regularized linear logistic regressor <br>
					
						on the training data with SGD.<br>
					
						Log. Reg. features used as input to classifier (trained separately).<br>
					
						<br>
					
						Also,<br>
					
						Sq. Root Sampling is better than<br>
					
						Uniform sampling is better than<br>
					
						Natural sampling from the noisy dataset.<br>
					
				</div>

				<div class="content-body">

					<div class="content-body-left">
						<div class="content-body-left-title">
						Abstract
						</div>
						State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards “small”. Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.
					</div>

					<div class="content-body-right">
						<p class="content-body-right-para">
							<span class="detailname">Paper Authors</span>
							<br>
							
								Dhruv Mahajan,&nbsp;
							
								Ross Girshick,&nbsp;
							
								Vignesh Ramanathan,&nbsp;
							
								Kaiming He,&nbsp;
									
						</p>

						<p class="content-body-right-para">
							<span class="detailname">Authors Affiliation</span>
							<br> 
							Facebook
						</p>

						
						<p class="content-body-right-para">
							<span class="detailname">ImageNet Top-1 Acc</span> 
							<br>
							85.40%
						</p>
						
						<p class="content-body-right-para">
							<span class="detailname">COCO Object Detection mAP</span> 
							<br>
							45.20%
						</p>
						

						<p class="content-body-right-para">
							<span class="detailname">Tags</span> 
							<br>
							
								Image Classification;&nbsp;
							
								Object Detection;&nbsp;
							
								Transfer Learning;&nbsp;
							
						</p>

						<p class="content-body-right-para">
							<span class="detailname">Level</span>
							<br> 
							Advanced				
						</p>

						<span>
						<a class="read-more-link" target="_blank" href="https://arxiv.org/pdf/1805.00932v1.pdf">
							Paper Link
						</a>
						&nbsp;
						<a class="read-more-link" href="https://paperswithcode.com/paper/exploring-the-limits-of-weakly-supervised" target="_blank">
							Code
						</a>
						</span>
					</div>

				</div>

				<div class="content-footer">
					Paper Published on: 02 May 2018
					<br>
					Haiku added on: 11 Feb 2021
				</div>
			</div>


			<div class="excerpt-holder">
				<div class="excerpt-title">
					Paper excerpts to justify the haiku
				</div>
				
					<img class="excerpt-image" src="../.././papers/0000010/screenshots/0000010_01.png">
				
					<img class="excerpt-image" src="../.././papers/0000010/screenshots/0000010_02.png">
				
					<img class="excerpt-image" src="../.././papers/0000010/screenshots/0000010_03.png">
				
					<img class="excerpt-image" src="../.././papers/0000010/screenshots/0000010_04.png">
				
					<img class="excerpt-image" src="../.././papers/0000010/screenshots/0000010_05.png">
				
					<img class="excerpt-image" src="../.././papers/0000010/screenshots/0000010_06.png">
				
					<img class="excerpt-image" src="../.././papers/0000010/screenshots/0000010_07.png">
				
					<img class="excerpt-image" src="../.././papers/0000010/screenshots/0000010_08.png">
				
					<img class="excerpt-image" src="../.././papers/0000010/screenshots/0000010_09.png">
				
			</div>
		</div>


</html>